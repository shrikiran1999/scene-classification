{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670bcd0",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1bfc20a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Bag of Visual Words - Assignment 2\n",
    "\n",
    "     Instructor: Kris Kitani                   TAs: Sheng-Yu, Jinkun, Rawal, Arka, Rohan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b1d40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ba35ab89cdc3e39c5704e8e4e387627",
     "grade": false,
     "grade_id": "cell-2f0bf8de83a87eae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Theory Questions\n",
    "\n",
    "This section should include the visualizations and answers to specifically highlighted questions from P1 to P4. This section will be manually Graded "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7cae1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "199f0b423bb79cdeb8d761ce8f9df98d",
     "grade": false,
     "grade_id": "cell-d2e7501d5ec1729e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Q1.1.1 (5 Points WriteUp)\n",
    "What visual properties do each of the filter functions (See Figure below) pick up? You should group the filters into categories by its purpose/functionality. Also, why do we need multiple scales of filter responses? **Answer in the writeup. Answer in your write-up.**\n",
    "\n",
    "<img align=\"center\" src=\"figures/filters_image.png\" width=\"500\">\n",
    "<figcaption align=\"center\"><b>Figure1. The provided multi-scale filter bank</b></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e016fd5",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "deca7d19a5637c50906e02a8d6c4877f",
     "grade": true,
     "grade_id": "cell-f20eebb8abbd872b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "__Solution__\n",
    "\n",
    "The 4 types of filters we have used to extract image features are (in order of rows in the above image):\n",
    "\n",
    "1. Gaussian: smoothes the image, hence removing noise and fine details. lower sensitivity to noise makes the image feature more representative of the class to which that image belongs.\n",
    "\n",
    "2. Laplacian of Gaussian: this filter can be used for edge detection as well as for blob detection (taking the example of the sunflower image discussed in class, this filter could be used for detecting the center of the sunflower )\n",
    "\n",
    "3. derivative of Gaussian in the $x$ direction: to get vertical edge information\n",
    "\n",
    "4. derivative of Gaussian in the $y$ direction: to get horizontal edge information\n",
    "\n",
    "Filters of mutliple scales are used since features might be of different scales in the images. For ex., sharp vs thick edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4dca99",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c4b37",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](1_1_2_filters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f98fa",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d15e9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](1_2_1_1.png)\n",
    "\n",
    "identifies the tip of the dune\n",
    "\n",
    "![](1_2_1_2.png)\n",
    "\n",
    "identifies the gaps between the balls in the yellow game machine. also identifies corners of the rectangular shaped game machines.\n",
    "\n",
    "![](1_2_1_3.png)\n",
    "\n",
    "identifies the sharp tips on heads and tails of the fishes. identifies only some fishes, especially the ones in the bright region, which means the detector performs better in brighter regions (more contrast)\n",
    "\n",
    "![](1_2_1_4.png)\n",
    "\n",
    "identifes some corners in the windmill, but mostly identifies the corners in the trees. \n",
    "\n",
    "![](1_2_1_5.png)\n",
    "\n",
    "many corners are missed, probably because of the low brightness of the image (lower contrast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c687526",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b6856c73e0ec7654f58f2bfa2e8d7f",
     "grade": false,
     "grade_id": "cell-f8136fffb67fc66f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q1.3.1 (5 Points WriteUp)\n",
    "\n",
    "Visualize three wordmaps of images from any one of the category. **Include these in your write-up, along with the original RGB images. Include some comments on these visualizations: do the “word” boundaries make sense to you?**. We have provided helper function to save and visualize the resulting wordmap in the util.py file. They should look similar to the ones in Figure 2.\n",
    "\n",
    "<img align=\"center\" src=\"./figures/textons.jpg\" width=\"800\">\n",
    "<figcaption align = \"center\"><b>Figure 2. Visual words over images. You will use the spatially un-ordered distribution of visual words in a region (a bag of visual words) as a feature for scene classification, with some coarse information provided by spatial pyramid matching [2]</b></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43924403",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0337fe508756898a619bf94bf9c0fc60",
     "grade": true,
     "grade_id": "cell-806d8af4e95d61d2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. ![](Q1_1_2_0.png)    ![](data\\desert\\sun_aaqyzvrweabdxjzo.jpg)\n",
    "\n",
    "2. ![](Q1_1_2_1.png)    ![](./data/laundromat/sun_aakuktqwgbgavllp.jpg)\n",
    "\n",
    "3. ![](Q1_1_2_2.png)    ![](./data/kitchen/sun_aahxnzrpowefyvrp.jpg)\n",
    "\n",
    "The word boundaries do make sense to an extent. It is clear that the word boundaries are able to partition objects. For example, in the first image, a clear distinction can be seen between the mountain and the background.\n",
    "In the second image, the words boundaries successfully demarcate the circular shapes which are washing machine doors; this feature is common in a laundry and hence will help in identifying similar images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a9b7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "456c33594131e9a50908bfcc7b703f4e",
     "grade": false,
     "grade_id": "cell-2cf410e4507cf87f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q2.1\n",
    "**For 5 Images, include their visual word maps and histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25046b96",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aa3b80c369eccfad7ee7aa4509d0770",
     "grade": true,
     "grade_id": "cell-f8873a304123ee24",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "![](Q1_1_2_0.png)   ![](2_1_0.png) \n",
    "\n",
    "![](Q1_1_2_1.png)   ![](2_1_1.png)\n",
    "\n",
    "![](Q1_1_2_2.png)   ![](2_1_2.png)\n",
    "\n",
    "![](Q1_1_2_3.png)   ![](2_1_3.png)\n",
    "\n",
    "![](Q1_1_2_4.png)   ![](2_1_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef0616",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd2391f7ef74f5d6be7b06374f7d41de",
     "grade": false,
     "grade_id": "cell-f11c4f53168fabbf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.1 \n",
    "Submit the visualization of Confusion Matrix and the Accuracy value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79e331",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "189f94a62bb1a83b1fb8a933028e5306",
     "grade": true,
     "grade_id": "cell-a67d219e82ac3ea5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](3_conf_matrix.png)\n",
    "\n",
    "Accuracy: 66.875%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e9e9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5219c1bfeff05a50e6c52fb438b4f120",
     "grade": false,
     "grade_id": "cell-c77fa30dd0533616",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Q3.1.2 (5 points WriteUp):\n",
    "\n",
    "As there are some classes/samples that are more difficult to classify than the rest using the bags-of-words approach, they are more easily classified incorrectly into other categories. **List some of these classes/samples and discuss why they are more difficult.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515d163",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aeaa24fd50837bd0204227238782e828",
     "grade": true,
     "grade_id": "cell-fe8e3fd47e21e13c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "__Answer__\n",
    "\n",
    "Class 5 (Laundromat) and Class 3 (highway) seem to have performed the worst. This can be attributed to the dense nature of a these classes, for ex: a kitchen has many small objects and doesn't have many defining features (in comparison to the best well performing classes like 2 (desert), which has very few unique objects, mostly the dunes and the sky). Also, the highway might have many vehicles and other objects on the side of the road, and these other objects could vary between different images in the same class.\n",
    "Since we are selecting features from only a few points (Harris corner points), the bag-of-words approach misses a lot of information, espcially in classes where many objects are present in one image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a1c2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2da804b4ebd6680ded96429b206041e1",
     "grade": false,
     "grade_id": "cell-a0d4cf029c9816a6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.3 [Extra Credit](10 points) Manually Graded:\n",
    "\n",
    "Now that you have seen how well your recognition system can perform on a set of real images, you can experiment with different ways of improving this baseline system. \n",
    "\n",
    "Include the changes, modification you made and the impact it had on accuracy.\n",
    "\n",
    "Tune the system you build to reach around 65\\% accuracy on the provided test set (``data/test_data.npz``). **In your writeup, document what you did to achieve such performance: (1) what you did, (2) what you expected would happen, and (3) what actually happened.** Also, include a file called ``custom.py/ipynb`` for running your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af5f99",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b04b30519c4d03f7064609fb83fb0d7",
     "grade": true,
     "grade_id": "cell-b7979e73bac0c915",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5a2d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c65c949456220674e7a42ef4653c40ce",
     "grade": false,
     "grade_id": "cell-0ab5de6e6222b473",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.4 [Extra Credit] (5 points write up):\n",
    "**GIST feature descriptor:** As introduced during the lecture, GIST feature descriptor is a feature extractor based on Gabor Filters. When we apply it to images, we have to implement the 2D Gabor Filters as described below\n",
    "\n",
    "<img align=\"center\" src=\"figures/gist.png\" width=\"800\">\n",
    "\n",
    "<font color=\"blue\">**In your writeup: How does GIST descriptor affect the performance? Better or worse? Explain your reasoning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481dcf3",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91cebc6bce44b037e5405cebd599c2bb",
     "grade": true,
     "grade_id": "cell-8949e75ea938cd42",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945a778",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b0bd3e412021431ff9479e9e99f5084",
     "grade": false,
     "grade_id": "cell-5a254c9a47e7f561",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q4.2.1 (2 points write up)\n",
    "**Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than classical BoW - why do you think that is?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8bba2",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12da5faa2c3fc2315a366648c12b87d4",
     "grade": true,
     "grade_id": "cell-c383f7a8536d254d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](4_conf_matrix.png)\n",
    "\n",
    "Accuracy: 97.5%\n",
    "\n",
    "The results are far better than classical BoW approach. \n",
    "\n",
    "The VGG network is a deep neural network with many layers. As we go from the first hidden layer to deeper layers in this network, the layer learns more and more sophisticated features. The VGG network being a Convolutional Neural Network-based model, it is translation invariant. Also, since the network has mulitple layers where the image is downsampled, the network is also scale invariant. The BoW approach is not sophisticated enough as compared to a Deep Neural Network such as VGG. Also, we only take features from a few points in the image during SPM, hence lot of information is lost, unlike VGG which extracts features from every pixel in the image. Hence the classical BoW approach has lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd52f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a04a7ff6ad689e2079f8c0ed82baa3c",
     "grade": false,
     "grade_id": "cell-81f8a97ac34fe774",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q4.3.2 [Extra Credit] (2 points write up)\n",
    "**Report the confusion matrix and accuracy for your ViT results in your write-up. Can you comment in your writeup on whether the results are better or worse than VGG - why do you think that is? A short answer is okay.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db840790",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "420edc8366b3e0d107b30a2d677a6df7",
     "grade": true,
     "grade_id": "cell-dc8ff4b969a16c8a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86969d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4e09d015af0ae2a101a7f03fcc8a9",
     "grade": false,
     "grade_id": "cell-39235682903e017c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[1]  James Hays and Alexei A Efros. Scene completion using millions of photographs.ACM Transactions onGraphics (SIGGRAPH 2007), 26(3), 2007.\n",
    "\n",
    "[2]  S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recogniz-ing natural scene categories.  InComputer Vision and Pattern Recognition (CVPR), 2006 IEEE Conferenceon, volume 2, pages 2169–2178, 2006.\n",
    "\n",
    "[3]  Jian xiong Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recogni-tion from abbey to zoo.2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,pages 3485–3492, 2010.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4d7dd",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c21fd19af84997c7324eb40f3f35b9c516eb0e316e912022307cacada437db6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
