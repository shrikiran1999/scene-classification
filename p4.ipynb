{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bc3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccc6827",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Bag of Visual Words - Assignment 2\n",
    "\n",
    "     Instructor: Kris Kitani                   TAs: Sheng-Yu, Jinkun, Rawal, Arka, Rohan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30de23a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa34e1bccadddd4af6b000d97fc91852",
     "grade": false,
     "grade_id": "cell-ab714820928ab812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from skimage import io\n",
    "import skimage.transform\n",
    "import os,time\n",
    "import util\n",
    "import multiprocess\n",
    "import threading\n",
    "import queue\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d50839",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5364b7bd8eec5796ece1fae065f6a05c",
     "grade": false,
     "grade_id": "cell-0943543d897db3c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## For Autograding P4, ensure uploading `trained_conf_matrix.npy` and `trained_system_deep.npz`. \n",
    "\n",
    "## For extra credit, ensure uploading `trained_conf_matrix_vit.npy` and `trained_system_vit.npz`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b983a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3df0d1eeb0e83e225049013b703cb55e",
     "grade": false,
     "grade_id": "cell-17340b02fdb1c6df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Deep Learning Features - An Alternative to ``Bag of Words``\n",
    "\n",
    "As we have discussed in class, another powerful method for scene classification in computer vision is the employment of convolutional neural networks (CNNs) - sometimes referred to generically as $deep learning$. It is important to understand how previously trained (pretrained) networks can be used as another form of feature extraction, and how they relate to classical Bag of Words (BoW) features. We will be covering details on how one chooses the network architecture and training procedures later in the course. For this question, however, we will be asking you to deal with the VGG-16 pretrained network. VGG-16 is a pretrained Convolutional Neural Network (CNN) that has been trained on approximately 1.2 million images from the ImageNet Dataset (``http://image-net.org/index``) by the Visual Geometry Group (VGG) at University of Oxford. The model can classify images into a 1000 object categories (e.g. keyboard, mouse, coffee mug, pencil).\n",
    "\n",
    "One lesson we want you to take away from this exercise is to understand the effectiveness of $deep$ $features$ for general classification tasks within computer vision - even when those features have been previously trained on a different dataset (i.e. ImageNet) and task (i.e. object recognition). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0762f44",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afb19706afe8d0615f44a208e29ff6bd",
     "grade": false,
     "grade_id": "cell-4d7a2d7da9ad6236",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Extracting Deep Features\n",
    "\n",
    "To complete this question, you need to install the ``torchvision`` library from Pytorch, a popular Python-based deep learning library.\n",
    "If you are using the Anaconda package manager (``https://www.anaconda.com/download/``), this can be done with the following command:\n",
    "```\n",
    "            conda install pytorch torchvision -c pytorch\n",
    "```\n",
    "To check that you have installed it correctly, make sure that you can ``import torch`` in a Python interpreter without errors.\n",
    "Please refer to ``https://pytorch.org/`` for more detailed installation instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77d7aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea96722fcf18543140b0ec3d16bd9e2d",
     "grade": false,
     "grade_id": "cell-1086d0481a485a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q4.1.1 (25 Points)\n",
    "\n",
    "We want to extract out deep features corresponding to the convolutional layers of theVGG-16 network.  In this problem, we will use the trained weights from the VGG network directly, but implement our own operations. To load the network, use the line\n",
    "```\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "```\n",
    "followed by ``vgg16.eval()``\n",
    "The latter line ensures that the VGG-16 network is in evaluation mode, not training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1e034",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92298155f306c3a6810a855a830ab225",
     "grade": false,
     "grade_id": "cell-0852d8a76345d947",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "path_img = \"./data/aquarium/sun_aztvjgubyrgvirup.jpg\"\n",
    "\n",
    "image = io.imread(path_img)\n",
    "\n",
    "image = image.astype('float')/255\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3be0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28e71afeaf91366424b2b9faf92e5a55",
     "grade": false,
     "grade_id": "cell-8b240fc2fc30262a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We want you to complete a function that is able to output $VGG-16$ network outputs at the **fc7** layer in\n",
    "```\n",
    "    def extract_deep_feature(x,vgg16_weights):\n",
    "```\n",
    "where ``x`` refers to the input image and ``vgg16_weights`` is a structure containing the CNN's network parameters. In this function you will need to write sub-functions ``multichannel_conv2d``, ``relu``, ``max_pool2d``, and ``linear`` corresponding to the fundamental elements of the CNN: multi-channel convolution, rectified linear units (ReLU), max pooling, and fully-connected weighting.\n",
    "\n",
    "We have provided a helper function ``util.get_VGG16_weights()`` that extracts the weight parameters of VGG-16 and its meta information. The returned variable is a numpy array of shape $L\\times 3$, where $L$ is the number of layers in VGG-16. The first column of each row is a string indicating the layer type. The second/third columns may contain the learned weights and biases, or other meta-information (\\eg kernel size of max-pooling). Please refer to the function docstring for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1a7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "126bd1545c46831c642e8428f6660a7c",
     "grade": false,
     "grade_id": "cell-5cbd9317d3fa3557",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In order to build the ``extract_deep_feature`` function, you should run a for-loop through each layer index until layer **fc7**, which corresponds to **the second linear layer** (Refer to VGG structure to see where **fc7** is). **Remember**: the output of the preceding layer should be passed through as an input to the next.\n",
    "\n",
    "Details on the sub-functions needed for the ``extract_deep_feature`` function can be found below.\n",
    "\n",
    "Please use ``scipy.ndimage.convolve`` and ``numpy`` functions to implement these functions instead of using pytorch. Please keep speed in mind when implementing your function, for example, using double for loop over pixels is not a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259dc42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "265efb8ebc1b1970c7cdb98ffd8c36b3",
     "grade": false,
     "grade_id": "cell-536cfed312995b11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``multichannel_conv2d(x,weight,bias)``:\n",
    "\n",
    "a function that will perform multi-channel 2D convolution which can be defined as follows, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}^{(j)} = \\sum_{k=1}^{K} \\begin{bmatrix} \\mathbf{x}^{(k)} * \\mathbf{h}^{(j,k)} \\end{bmatrix} + \\mathbf{b}[j] \n",
    "\\end{equation}\n",
    "\n",
    "where $*$ denotes $2D$ convolution, $\\mathbf{x} = \\{ \\mathbf{x}^{(k)} \\}_{k=1}^{K}$ is our vectorized $K$-channel input signal, $\\mathbf{h} = \\{ \\mathbf{h}^{(j,k)} \\}_{k=1,j=1}^{K,J}$ is our $J \\times K$ set of vectorized convolutional filters and $\\mathbf{r} = \\{ \\mathbf{y}^{(j)} \\}_{j=1}^{J}$ is our $J$ channel vectorized output response. Further, unlike traditional single-channel convolution CNNs often append a bias vector $\\mathbf{b}$ whose $J$ elements are added to the $J$ channels of the output response. \n",
    "\n",
    "To implement ``multichannel_conv2d``, you can use the Scipy convolution function directly with for loops to cycle through the filters and channels (``scipy.ndimage.convolve()``). All the necessary details concerning the number of filters ($J$), number of channels ($K$), filter weights ($\\mathbf{h}$) and biases ($\\mathbf{b}$) can be inferred from the shapes/dimensions of the weights and biases. Notice that pytorch's convolution function actually does correlation, so to get similar answer as pytroch with scipy, you need to flip the kernel on both axes using ``np.flip()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afee3b08",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfafc16377dad8df84358ed123409a7d",
     "grade": false,
     "grade_id": "cell-39e262deb285001e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multichannel_conv2d(x,weight,bias):\n",
    "    '''\n",
    "    Performs multi-channel 2D convolution.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,input_dim)\n",
    "    * weight: numpy.ndarray of shape (output_dim,input_dim,kernel_size,kernel_size)\n",
    "    * bias: numpy.ndarray of shape (output_dim)\n",
    "\n",
    "    [output]\n",
    "    * feat: numpy.ndarray of shape (H,W,output_dim)\n",
    "    '''\n",
    "    h, w, input_dims = x.shape\n",
    "    output_dims = weight.shape[0] # no of kernels\n",
    "    final_res = np.zeros((h, w, output_dims))\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> for 2D convolution we need to use np.fliplr and np.flipud\n",
    "    2.> can use scipy.ndimage.convolve with the flipped kernel\n",
    "    3.> don't forget to add the bias\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    for i in range(output_dims):\n",
    "        for j in range(input_dims):\n",
    "            kernel = weight[i,j,:,:]\n",
    "            kernel = np.flipud(np.fliplr(kernel))\n",
    "            per_channel_per_filter_op = scipy.ndimage.convolve(x[:,:,j], kernel, mode='constant', cval=0.0) # (h.w)\n",
    "            final_res[:,:,i]+=per_channel_per_filter_op\n",
    "        final_res[:,:,i]+=bias[i]\n",
    "        \n",
    "        \n",
    "    # raise NotImplementedError()\n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e5939",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ae1c551b9359d94adf085873f98e4ba",
     "grade": false,
     "grade_id": "cell-244ad83440a13a11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "```relu(x)```:\n",
    "\n",
    "a function that shall perform the Rectified Linear Unit (ReLU) which can be defined mathematically as,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{ReLU}(x) = \\max_{x}(x, 0)\n",
    "\\end{equation}\n",
    "\n",
    "and is applied independently to each element of the matrix/vector $x$ passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547e2ce4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0e9f9b71377bf8a52d6526abf7834f",
     "grade": false,
     "grade_id": "cell-9129e8637bed35d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Rectified linear unit.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    y = np.maximum(x, np.zeros_like(x))\n",
    "    # raise NotImplementedError()\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec570ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850072d50afe0d0486db01d3c3dc028e",
     "grade": false,
     "grade_id": "cell-bbe9e052ed717774",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``max_pool2d(x,size):``\n",
    "\n",
    "a function that shall perform max pooling over $x$ using a receptive field of size $size$ $\\times$ $size$ (we assume a square receptive field here for simplicity).\n",
    "\n",
    "  If the function receives a multi-channel input, then it should apply the max pooling operation across each input channel independently.\n",
    "  \n",
    "(Hint: making use of smart array indexing can drastically speed up the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77126b1a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc3d9525fa5233cbcb1a2288e5408d1d",
     "grade": false,
     "grade_id": "cell-c1438c61d632ab1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_pool2d(x,size):\n",
    "    '''\n",
    "    2D max pooling operation.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,input_dim)\n",
    "    * size: pooling receptive field\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray of shape (H/size,W/size,input_dim)\n",
    "    '''\n",
    "    h, w, dims = x.shape\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> estimate the shape you need to apply the pooling operation.\n",
    "    2.> We can smart fill the padding with np.nan and then use np.nanmax to select the max (avoiding nan)\n",
    "    3.> We can input the grid (start_x:end_x, start_y:end_y, dim) as smart array indexing to np.nanmax\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # print(\"x shape:\", str(x.shape))\n",
    "    if h%size==0:\n",
    "        new_h = int(h/size)\n",
    "        pad_h = 0\n",
    "    else:\n",
    "        # print('here')\n",
    "        new_h = int(h//size) + 1\n",
    "        pad_h = int(size - h%size)\n",
    "    if w%size==0:\n",
    "        new_w = int(w/size)\n",
    "        pad_w = 0\n",
    "    else:\n",
    "        # print('here')\n",
    "        new_w = int(w//size) + 1\n",
    "        pad_w = int(size - w%size)\n",
    "\n",
    "    pooled_arr = np.zeros((new_w, new_h, dims))\n",
    "\n",
    "     \n",
    "    for i in range(dims):\n",
    "        x_padded = np.pad(x[:,:,i], ((0, pad_h), (0, pad_w)), 'constant', constant_values = np.nan)\n",
    "        k = 0\n",
    "        for m in range(0, x_padded.shape[0], size):\n",
    "            l=0\n",
    "            for n in range(0, x_padded.shape[1], size):\n",
    "                pooled_arr[k, l, i] = np.nanmax(x_padded[m:m+size, n:n+size])\n",
    "\n",
    "                l+=1\n",
    "            k+=1\n",
    "    # raise NotImplementedError()\n",
    "    # print(\"x shape after maxpool:\", str(pooled_arr.shape))\n",
    "    \n",
    "    return pooled_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f934916",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a30a5e18db579607d17478cb28066cc",
     "grade": false,
     "grade_id": "cell-a0d2f5e224adba0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``linear(x,W,b):``\n",
    "\n",
    "a function that will compute a node vector where each element is a linear combination of the input nodes, written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}[j] = \\sum_{k=1}^{K}\\mathbf{W}[j,k] \\mathbf{x}[k] + \\mathbf{b}[j] \n",
    "\\end{equation}\n",
    "\n",
    "or more succinctly in vector form as $\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$ - where $\\mathbf{x}$ is the $(K \\times 1)$ input node vector, $\\mathbf{W}$ is the $(J \\times K)$ weight matrix, $\\mathbf{b}$ is the $(J \\times 1)$ bias vector and $\\mathbf{y}$ is the~$(J \\times 1)$ output node vector.\n",
    "\n",
    "You should not need for-loops to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10c3744",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83ed8dacb1fb983d4d1f336de7601abc",
     "grade": false,
     "grade_id": "cell-aee1e87151903086",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,W,b):\n",
    "    '''\n",
    "    Fully-connected layer.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (input_dim)\n",
    "    * weight: numpy.ndarray of shape (output_dim,input_dim)\n",
    "    * bias: numpy.ndarray of shape (output_dim)\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray of shape (output_dim)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    # print(W.shape) # (4096, 25088)\n",
    "    # print(x.shape) # (7, 7, 512)\n",
    "    # print(b.shape) # (4096,)\n",
    "    # print(np.matmul(W, x.flatten()).shape)\n",
    "    # print(x.shape)\n",
    "    try:\n",
    "        x = np.transpose(x, (2,0,1)).flatten() # some layers pass a flattened array; also need to transpose before flattening\n",
    "                                               # to match vgg features order\n",
    "    except:\n",
    "        # print(x.shape)\n",
    "        pass\n",
    "    y = np.matmul(W,x) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336b9bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "146fa6565ac543a5086ff5c24eeb6585",
     "grade": false,
     "grade_id": "cell-54047ba1b7802e8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should ignore all ``DropoutLayer`` you encounter; they're functional only during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b47ce6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a6f29fbda60148eaaac3fb8e7f4b89d",
     "grade": false,
     "grade_id": "cell-5491f084ad832f2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "VGG-16 assumes that all input imagery to the network is resized to $224 \\times 224$ with the three color channels preserved (use ``skimage.transform.resize()`` to do this before passing any imagery to the network). And be sure to normalize the image using suggested mean and std before extracting the feature:\n",
    "```\n",
    "                                        mean=[0.485,0.456,0.406]}\n",
    "                                        std=[0.229,0.224,0.225]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96a0ffa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9656d91025bba1426dd161ada89c41ec",
     "grade": false,
     "grade_id": "cell-7d0ed28aa0f1bffb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    '''\n",
    "    Preprocesses the image to load into the prebuilt network.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W,3)\n",
    "\n",
    "    [output]\n",
    "    * image_processed: torch.array of shape (3,H,W)\n",
    "    '''\n",
    "\n",
    "    # ----- TODO -----\n",
    "    \n",
    "    if(len(image.shape) == 2):\n",
    "        image = np.stack((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape == 3 and image.shape[2] == 1):\n",
    "        image = np.concatenate((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape[2] == 4):\n",
    "        image = image[:, :, 0:3]\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Resize the image (look into skimage.transform.resize)\n",
    "    2.> normalize the image\n",
    "    3.> convert the image from numpy to torch\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    mean=[0.485,0.456,0.406]\n",
    "    std=[0.229,0.224,0.225]\n",
    "    image_resized = skimage.transform.resize(image, (224, 224))\n",
    "    channels = image.shape[2]\n",
    "    channels_normalized = []\n",
    "    for c in range(channels):\n",
    "        # print(image_resized[:,:,c])\n",
    "        pix_norm = (image_resized[:,:,c]-mean[c])/std[c]\n",
    "        # print(pix_norm)\n",
    "        channels_normalized.append(pix_norm)\n",
    "    \n",
    "    # print(len(channels_normalized))\n",
    "    # for channel in channels_normalized:\n",
    "    #     channel = skimage.transform.resize(channel, (224,224))\n",
    "    image_processed = np.stack((channels_normalized))\n",
    "    image_processed = torch.from_numpy(image_processed)\n",
    "\n",
    "    # raise NotImplementedError()\n",
    "    return image_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb456f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8725f2cb95e93f82c875bce123ee6c4c",
     "grade": false,
     "grade_id": "cell-1cede50aa78672ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For efficiency you should check that each sub-function is working properly before putting them all together - otherwise it will be hard to track any errors. To compare your implementation with pytroch, you should compare the extracted features between your ``extract_deep_feature``  and the pre-trained VGG-16 network.  ``evaluate_deep_extractor`` should come in handy in comparing the result of the two extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc3e81d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c391c1be80d17baab19771e8a946935",
     "grade": false,
     "grade_id": "cell-27d0fcec8607c9f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_deep_feature(x, vgg16_weights):\n",
    "    '''\n",
    "    Extracts deep features from the given VGG-16 weights.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,3)\n",
    "    * vgg16_weights: list of shape (L,3)\n",
    "\n",
    "    [output]\n",
    "    * feat: numpy.ndarray of shape (K)\n",
    "    '''\n",
    "    \n",
    "    feat = np.copy(x)\n",
    "    # YOUR CODE HERE\n",
    "    linear_count = 0\n",
    "    for layer in vgg16_weights:\n",
    "        name = layer[0]\n",
    "        if name==\"conv2d\":\n",
    "            _, weight, bias = layer\n",
    "            feat = multichannel_conv2d(feat,weight,bias)\n",
    "        elif name==\"relu\":\n",
    "            feat = relu(feat)\n",
    "        elif name==\"maxpool2d\":\n",
    "            _, k_size = layer\n",
    "            feat = max_pool2d(feat, k_size)\n",
    "        elif name==\"linear\":\n",
    "            _, weight, bias = layer\n",
    "            feat = linear(feat, weight, bias)\n",
    "            linear_count+=1\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if linear_count==2:\n",
    "            break\n",
    "        \n",
    "\n",
    "    # raise NotImplementedError()\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23803b61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1957f46435c9183a7cb810f8b189c160",
     "grade": false,
     "grade_id": "cell-83df22bcc6adc770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_deep_extractor(img, vgg16):\n",
    "    '''\n",
    "    Evaluates the deep feature extractor for a single image.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W,3)\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "\n",
    "    [output]\n",
    "    * diff: difference between the two feature extractor's result\n",
    "    '''\n",
    "    \n",
    "    vgg16_weights = util.get_VGG16_weights()\n",
    "    img_torch = preprocess_image(img)\n",
    "    # print(img_torch.shape)\n",
    "    # print(np.transpose(img_torch.numpy(), (1,2,0)).shape)\n",
    "    feat = extract_deep_feature(np.transpose(img_torch.numpy(), (1,2,0)), vgg16_weights)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\n",
    "        vgg_feat_feat = vgg16.features(img_torch[None, ])\n",
    "        vgg_feat_feat = vgg_classifier(vgg_feat_feat.flatten())\n",
    "    \n",
    "    return np.sum(np.abs(vgg_feat_feat.numpy() - feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b68be1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4.498847475109535e-12\n"
     ]
    }
   ],
   "source": [
    "# NOTE: comment out the lines below before submitting to gradescope\n",
    "# Visible test cases (for debugging)\n",
    "path_img = \"./data/aquarium/sun_aztvjgubyrgvirup.jpg\"\n",
    "image = io.imread(path_img)\n",
    "image = image.astype('float') / 255\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "vgg16.eval()\n",
    "error = evaluate_deep_extractor(image, vgg16)\n",
    "\n",
    "# This error should be less than 1e-10\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5176d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f71d40e5082c99099739ccd3b77548d",
     "grade": false,
     "grade_id": "cell-b841ba750b9c0b46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Building a Visual Recognition System: Revisited\n",
    "\n",
    "We want to compare how useful deep features are in a visual recognition system. Since the speed of the function ``` scipy.ndimage.convolve``` is not ideal, you can use the pytroch VGG-16 network directly (refer to the helper function ```evaluate_deep_extractor``` on how to use the pre-trained network as feature extractor).\n",
    "\n",
    "#### Q4.2.1 (5 Points Autograder + WriteUp):\n",
    "Implement the functions\n",
    "```\n",
    "                    def build_recognition_system(vgg16):\n",
    "```\n",
    "and\n",
    "```\n",
    "                    def eval_recognition_system(vgg16)}\n",
    "```\n",
    "both of which takes the pretrained VGG-16 network as the input arguments. \n",
    "\n",
    "The former function should produce ``trained_system_deep.npz`` as the output. \n",
    "\n",
    "Included will be:\n",
    "* $features$ : a $N \\times  K$ matrix containing all the deep features of the $N$ training images in the data set.\n",
    "* $labels$ : an $N$ vector containing the labels of each of the images. ($features[i]$ will correspond to label $labels[i]$).\n",
    "\n",
    "The latter function should produce the confusion matrix, as with the previous question.\n",
    "\n",
    "Instead of using the histogram intersection similarity, write a function to just use the negative Euclidean distance (as larger values are more similar).\n",
    "\n",
    "**Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than classical BoW - why do you think that is?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29eef544",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8cf20826e56c7b805643fb5e106171b",
     "grade": false,
     "grade_id": "cell-998a996bb091f4c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_image_feature(args):\n",
    "    '''\n",
    "    Extracts deep features from the prebuilt VGG-16 network.\n",
    "    This is a function run by a subprocess.\n",
    "    [input]\n",
    "    * i: index of training image\n",
    "    * image_path: path of image file\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "    \n",
    "    [output]\n",
    "    * feat: evaluated deep feature\n",
    "    '''\n",
    "    i, image_path, vgg16 = args\n",
    "    image = io.imread(image_path) / 255\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Think along the lines of evaluate_deep_extractor\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    vgg16_weights = util.get_VGG16_weights() # NOT REQUIRED, MODEL ALREADY PASSED AS ARGUMENT\n",
    "    img_torch = preprocess_image(image)\n",
    "    # print(img_torch.shape)\n",
    "    # print(np.transpose(img_torch.numpy(), (1,2,0)).shape)\n",
    "    # feat = extract_deep_feature(np.transpose(img_torch.numpy(), (1,2,0)), vgg16_weights)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\n",
    "        vgg_feat_feat = vgg16.features(img_torch[None, ])\n",
    "        vgg_feat_feat = vgg_classifier(vgg_feat_feat.flatten())\n",
    "    \n",
    "    # return np.sum(np.abs(vgg_feat_feat.numpy() - feat))\n",
    "    # raise NotImplementedError()\n",
    "    feat = vgg_feat_feat.numpy()\n",
    "    return [i,feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab97e2d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "690e046304f5925ce2bbfbae43d83828",
     "grade": false,
     "grade_id": "cell-df193a9dbf349af9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "def build_recognition_system(vgg16, num_workers=8):\n",
    "    '''\n",
    "    Creates a trained recognition system by generating training features from all training images.\n",
    "\n",
    "    [input]\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "    * num_workers: number of workers to process in parallel\n",
    "\n",
    "    [saved]\n",
    "    * features: numpy.ndarray of shape (N,K)\n",
    "    * labels: numpy.ndarray of shape (N)\n",
    "    '''\n",
    "\n",
    "    train_data = np.load(\"./data/train_data.npz\", allow_pickle=True)\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Similar approach as Q1.2.2 and Q3.1.1 (create an argument list and use multiprocessing)\n",
    "    2.> Keep track of the order in which input is given to multiprocessing\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # train_data = np.load(\"./data/train_data.npz\")\n",
    "    # for f in train_data.files:\n",
    "    #     print(f)\n",
    "    labels = train_data['labels'] #(N,)\n",
    "    list_of_args = []\n",
    "    image_names = train_data['files']\n",
    "    num_images = image_names.shape[0]\n",
    "\n",
    "    for i in range(num_images):\n",
    "        full_image_name = './data/' + image_names[i]\n",
    "        list_of_args.append([i, full_image_name, vgg16])\n",
    "    \n",
    "    # print(num_images)\n",
    "    # compute_dictionary_one_image(list_of_args[0])\n",
    "    # f_name = 'tmp/00000.npy'\n",
    "    # filter_response = np.load(f_name)\n",
    "    # print(filter_response.shape)\n",
    "    \n",
    "    # with multiprocess.Pool(num_workers) as p:\n",
    "    #     features_list = p.map(get_image_feature, list_of_args)\n",
    "    features_list = []\n",
    "    for arg in list_of_args:\n",
    "        features_list.append(get_image_feature(arg))\n",
    "    print(len(features_list))\n",
    "    # raise NotImplementedError()\n",
    "    # ordered_features = [None] * len(features)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> reorder the features to their correct place as input\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    ordered_features = [feature for [idx, feature] in sorted(features_list, key = lambda x: x[0])]\n",
    "    # print(\"done\", ordered_features.shape)\n",
    "    \n",
    "    np.savez('trained_system_deep.npz', features=ordered_features, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "357651ab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca8342e3b0756df670a5dd389cc6fefa",
     "grade": false,
     "grade_id": "cell-c7bb13cc505da9dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_test_image(args):\n",
    "    # YOUR CODE HERE\n",
    "    i, image_path, vgg16, trained_features, train_labels = args\n",
    "    image = io.imread(image_path) / 255\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Think along the lines of evaluate_deep_extractor\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    vgg16_weights = util.get_VGG16_weights()\n",
    "    img_torch = preprocess_image(image)\n",
    "    # print(img_torch.shape)\n",
    "    # print(np.transpose(img_torch.numpy(), (1,2,0)).shape)\n",
    "    # feat = extract_deep_feature(np.transpose(img_torch.numpy(), (1,2,0)), vgg16_weights)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\n",
    "        vgg_feat_feat = vgg16.features(img_torch[None, ])\n",
    "        vgg_feat_feat = vgg_classifier(vgg_feat_feat.flatten())\n",
    "    \n",
    "    # return np.sum(np.abs(vgg_feat_feat.numpy() - feat))\n",
    "    # raise NotImplementedError()\n",
    "    N = trained_features.shape[0]\n",
    "    feat = vgg_feat_feat.numpy() #(K,)\n",
    "    feat_repeated = np.stack(([feat]*N)) # (N, K)\n",
    "    distances = np.linalg.norm((feat_repeated-trained_features), axis=1)\n",
    "    idx_min = np.argmin(distances)\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    pred_label = train_labels[idx_min]\n",
    "    return [i, pred_label]\n",
    "\n",
    "\n",
    "def evaluate_recognition_system(vgg16, num_workers=8):\n",
    "    '''\n",
    "    Evaluates the recognition system for all test images and returns the confusion matrix.\n",
    "\n",
    "    [input]\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "    * num_workers: number of workers to process in parallel\n",
    "\n",
    "    [output]\n",
    "    * conf: numpy.ndarray of shape (8,8)\n",
    "    * accuracy: accuracy of the evaluated system\n",
    "    '''\n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) Students can write helper functions (in this cell) to use multi-processing\n",
    "    '''\n",
    "    test_data = np.load(\"./data/test_data.npz\", allow_pickle=True)\n",
    "\n",
    "    # ----- TODO -----\n",
    "    trained_system = np.load(\"trained_system_deep.npz\", allow_pickle=True)\n",
    "    image_names = test_data['files']\n",
    "    test_labels = test_data['labels']\n",
    "\n",
    "    trained_features = trained_system['features']\n",
    "    train_labels = trained_system['labels']\n",
    "\n",
    "    print(\"Trained features shape: \", trained_features.shape)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> [Important] Can write a helper function in this cell of jupyter notebook for multiprocessing\n",
    "    \n",
    "    2.> Helper function will compute the vgg features for test image (get_image_feature) and find closest\n",
    "        matching feature from trained_features.\n",
    "    \n",
    "    3.> Since trained feature is of shape (N,K) -> smartly repeat the test image feature N times (bring it to\n",
    "        same shape as (N,K)). Then we can simply compute distance in a vectorized way. np.stack(([test_festure]*N))\n",
    "    \n",
    "    4.> Distance here can be sum over (a-b)**2\n",
    "    \n",
    "    5.> np.argmin over distance can give the closest point\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    list_of_args = []\n",
    "    num_images = image_names.shape[0]\n",
    "\n",
    "    for i in range(num_images):\n",
    "        full_image_name = './data/' + image_names[i]\n",
    "        list_of_args.append([i, full_image_name, vgg16, trained_features, train_labels])\n",
    "    \n",
    "    # print(num_images)\n",
    "    # compute_dictionary_one_image(list_of_args[0])\n",
    "    # f_name = 'tmp/00000.npy'\n",
    "    # filter_response = np.load(f_name)\n",
    "    # print(filter_response.shape)\n",
    "    \n",
    "    # with multiprocess.Pool(num_workers) as p:\n",
    "    #     labels_list = p.map(evaluate_test_image, list_of_args[:10])\n",
    "    labels_list = []\n",
    "    for arg in list_of_args:\n",
    "        labels_list.append(evaluate_test_image(arg))\n",
    "        \n",
    "    print(len(labels_list))\n",
    "    # raise NotImplementedError()\n",
    "    # ordered_features = [None] * len(features)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> reorder the features to their correct place as input\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    ordered_labels = [label for [idx, label] in sorted(labels_list, key = lambda x: x[0])]\n",
    "    print(\"Predicted labels shape: \", len(ordered_labels))\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Compute the confusion matrix (8x8)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    conf_matrix = np.zeros((8,8))\n",
    "    for (actual_label, pred_label) in zip(test_labels, ordered_labels):\n",
    "        conf_matrix[actual_label][pred_label]+=1\n",
    "        \n",
    "    accuracy = np.trace(conf_matrix)/np.sum(conf_matrix)\n",
    "    \n",
    "    np.save(\"./trained_conf_matrix.npy\",conf_matrix)\n",
    "    return conf_matrix, accuracy\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cef91cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained features shape:  (1000, 4096)\n",
      "160\n",
      "Predicted labels shape:  160\n",
      "Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "# NOTE: comment out the lines below before submitting to gradescope\n",
    "### Run the code\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "vgg16.eval()\n",
    "\n",
    "# build_recognition_system(vgg16)\n",
    "conf_matrix, accuracy = evaluate_recognition_system(vgg16)\n",
    "# # We expect the accuracy to be greater than 0.9\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc33169",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87495a23809045f32a4753b8ccb39bec",
     "grade": false,
     "grade_id": "cell-b44920091955aca2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (Extra Credit) Extract ViT features\n",
    "\n",
    "**q4.3.1 (10 points)**\n",
    "For extra credit, we ask you to try out the features from a recently popular architecture -- Vision Transformers (ViTs).\n",
    "\n",
    "You'll learn more about ViT later in the course. As an overview, instead of convolutions, ViT treat image patches as tokens, and process the tokens via self-attention. For more details, feel free to check out the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (``https://arxiv.org/abs/2010.11929``).\n",
    "\n",
    "The goal in this problem is not to understand ViT. We expect you to learn how to extract features from an unseen architecture (e.g., a new model released straight out of arxiv). Here you'll learn how to:\n",
    "1. Iterate through the pytorch nn modules without digging too much into the source code.\n",
    "2. Learn another way to extract features -- `register_forward_hook()`\n",
    "\n",
    "In this problem, we'll be using an ImageNet-pretrained ViT from `pytorch_pretrained_vit`. In case you haven't installed it, run:\n",
    "\n",
    "```pip install pytorch_pretrained_vit```\n",
    "\n",
    "Below is a script that loads a pre-trained ViT and prints out all the submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d0178c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7062c6d191ebd6e507517c5eeb95c3b",
     "grade": false,
     "grade_id": "cell-0427f5646c9df273",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n",
      "ViT(\n",
      "  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (positional_embedding): PositionalEmbedding1D()\n",
      "  (transformer): Transformer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n",
      "Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "PositionalEmbedding1D()\n",
      "Transformer(\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): Block(\n",
      "      (attn): MultiHeadedSelfAttention(\n",
      "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pwff): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (3): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (4): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (5): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (6): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (7): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (8): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (9): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (10): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (11): Block(\n",
      "    (attn): MultiHeadedSelfAttention(\n",
      "      (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (pwff): PositionWiseFeedForward(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MultiHeadedSelfAttention(\n",
      "  (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "PositionWiseFeedForward(\n",
      "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "Linear(in_features=768, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_vit import ViT\n",
    "\n",
    "\n",
    "# loads a pretrained ViT\n",
    "vit_model = ViT('B_16_imagenet1k', pretrained=True)\n",
    "vit_model.eval()\n",
    "\n",
    "# iterate through the submodules and print out names\n",
    "for name, module in vit_model.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb9d41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "794ef7d63fe149d6ad49102a6f06abfa",
     "grade": false,
     "grade_id": "cell-f4161890fb1ae55f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`named_modules()` will recursively find out all the `nn.Modules` defined inside `vit_model`. For example: \n",
    "1. `vit_model` has children `patch_embedding`, `positional_embedding`, `transformer`, `norm`, `fc`.\n",
    "2. Inside the `transformer` module, `transformer` has a child `transformer.blocks`.\n",
    "3. `transformer.blocks` module has children `transformer.blocks.0`, `transformer.blocks.1`, etc.\n",
    "4. The logic applies recursively.\n",
    "\n",
    "Unfortunately, calling just `named_modules()` will not give you the information about the ordering each module is used.\n",
    "\n",
    "However, if you have a basic understanding of the architecture code, you can extract features quickly, regardless of how complicated the `forward()` function is. In ViT, we know that the `patch_embedding` and `positional_embedding` is passed into `transformer`, and the output of `transformer` is then passed into `norm` then `fc`.\n",
    "\n",
    "Here, we want to extract features from the output of the `transformer` module. We will use `register_forward_hook()` to do this. Please read the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=forward%20hook#torch.nn.Module.register_forward_hook) for details. Essentially, we want to \"hook\" a function that records the output of the `transformer` module, whenever `vit_model` is called. We'll define a class FeatureExtractor to do this.\n",
    "\n",
    "**Note:** The input image size for ViT will be 384x384, and normalization uses a different mean and std:\n",
    "```\n",
    "                                        mean=[0.5, 0.5, 0.5]\n",
    "                                        std=[0.5, 0.5, 0.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ea28d3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b0206d7b4eafe8f23aefa28ffe7c0a5",
     "grade": false,
     "grade_id": "cell-a6e05e47e0061913",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    '''\n",
    "    A class that takes in a nn.Module model and extracts feature from specified layer name.\n",
    "    '''\n",
    "    def __init__(self, model, layername='transformer'):\n",
    "        self.extracted_feature = None\n",
    "        self.model = model              # This will be vit_model in our case\n",
    "        self.layername = layername\n",
    "\n",
    "        # Apply hook to the transformer module\n",
    "        '''\n",
    "        HINTS:\n",
    "        1.> The for loop of named_modules() we provided will be useful here.\n",
    "        2.> Apply feature_extract_hook() to the transformer module using register_forward_hook()\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        for name, module in model.named_modules():\n",
    "            if name==self.layername:\n",
    "                ip = torch.rand(768)\n",
    "                op = torch.rand(1000, 768)\n",
    "                # self.feature_extract_hook(module)\n",
    "                module.register_forward_hook(self.feature_extract_hook(module, name))\n",
    "\n",
    "        \n",
    "\n",
    "    def feature_extract_hook(self, module, input, output):\n",
    "        '''\n",
    "        A function hook that extracts the module's output to the global variable `extracted_feature`\n",
    "\n",
    "        [input]\n",
    "        * module: module of interest\n",
    "        * input: input of the module\n",
    "        * output: output of the module\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        HINTS:\n",
    "        1.> You don't need to use all the arguments in this function.\n",
    "        2.> What you need to do in this function should be really simple.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        def hook(_, __, output):\n",
    "            self.extracted_feature = output\n",
    "        return hook\n",
    "\n",
    "    def extract_feature(self, img):\n",
    "        '''\n",
    "        Takes in an image, feed it to the model, and outputs the desired feature.\n",
    "        \n",
    "        [input]\n",
    "        * x: preprocessed image\n",
    "        \n",
    "        [output]\n",
    "        * feature: feature extracted from the specified layer name\n",
    "        '''\n",
    "        x = preprocess_image_vit(img).float()\n",
    "        \n",
    "        # simply run a forward pass of the model\n",
    "        with torch.no_grad():\n",
    "            self.model(x.unsqueeze(0))\n",
    "\n",
    "        # feature will be extracted in self.extracted_feature already, thanks to the hook\n",
    "        # you might wonder why we take only part of the output as feature\n",
    "        # this is because we are only using the \"class token\" as the feature\n",
    "        # for more details, please read the paper!\n",
    "        return self.extracted_feature.numpy()[0, 0]\n",
    "\n",
    "\n",
    "def preprocess_image_vit(image):\n",
    "    '''\n",
    "    Preprocesses the image to load into the prebuilt network.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W,3)\n",
    "\n",
    "    [output]\n",
    "    * image_processed: torch.array of shape (3,H,W)\n",
    "    '''\n",
    "\n",
    "    # ----- TODO -----\n",
    "    \n",
    "    if(len(image.shape) == 2):\n",
    "        image = np.stack((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape == 3 and image.shape[2] == 1):\n",
    "        image = np.concatenate((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape[2] == 4):\n",
    "        image = image[:, :, 0:3]\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> This function is essentially the same as the one you made before\n",
    "    2.> Make sure you change the image size, mean, and std.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    mean=[0.5, 0.5, 0.5]\n",
    "    std=[0.5, 0.5, 0.5]\n",
    "    image_resized = skimage.transform.resize(image, (384, 384))\n",
    "    channels = image.shape[2]\n",
    "    channels_normalized = []\n",
    "    for c in range(channels):\n",
    "        # print(image_resized[:,:,c])\n",
    "        pix_norm = (image_resized[:,:,c]-mean[c])/std[c]\n",
    "        # print(pix_norm)\n",
    "        channels_normalized.append(pix_norm)\n",
    "    \n",
    "    # print(len(channels_normalized))\n",
    "    # for channel in channels_normalized:\n",
    "    #     channel = skimage.transform.resize(channel, (224,224))\n",
    "    image_processed = np.stack((channels_normalized))\n",
    "    image_processed = torch.from_numpy(image_processed)\n",
    "\n",
    "    # raise NotImplementedError()\n",
    "    return image_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ae58e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcc66c67600645f39e024fe8e885cd3c",
     "grade": false,
     "grade_id": "cell-e7ac1eb05fbec3c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**q4.3.2 (5 points + Write up)** Now we can extract features from ViT to build our recognition system. The following code will be mostly the same as the one you did before.\n",
    "\n",
    "Write up: Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than VGG - why do you think that is? A short answer is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509c0f23",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efbde40fdfb4c984a97453164013ba76",
     "grade": false,
     "grade_id": "cell-2c426f5044472a37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def build_recognition_system_vit(vit_feat_extractor):\n",
    "    '''\n",
    "    Creates a trained recognition system by generating training features from all training images.\n",
    "\n",
    "    [input]\n",
    "    * vit_feat_extractor: feature extractor for ViT\n",
    "\n",
    "    [saved]\n",
    "    * features: numpy.ndarray of shape (N,K)\n",
    "    * labels: numpy.ndarray of shape (N)\n",
    "    '''\n",
    "\n",
    "    train_data = np.load(\"./data/train_data.npz\", allow_pickle=True)\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Similar approach as Q4.2.1\n",
    "    2.> Do a for loop here instead of multiprocessing (it can take around 30 min to run)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    labels = train_data['labels'] #(N,)\n",
    "    image_names = train_data['files']\n",
    "    num_images = image_names.shape[0]\n",
    "    features_list = []\n",
    "    for i in range(num_images):\n",
    "        full_image_name = './data/' + image_names[i]\n",
    "        image = io.imread(full_image_name) / 255\n",
    "        features_list.append(vit_feat_extractor.extract_feature(image))\n",
    "    print(len(features_list))\n",
    "    features = features_list\n",
    "\n",
    "    np.savez('trained_system_vit.npz', features=features, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9234f15",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7199e4b9d4d6508e069d662914a5ef8d",
     "grade": false,
     "grade_id": "cell-76c948b5431eabdb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_test_image(args):\n",
    "    # YOUR CODE HERE\n",
    "    i, image_path, vgg16, trained_features, train_labels = args\n",
    "    image = io.imread(image_path) / 255\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Think along the lines of evaluate_deep_extractor\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    vgg16_weights = util.get_VGG16_weights()\n",
    "    img_torch = preprocess_image(image)\n",
    "    # print(img_torch.shape)\n",
    "    # print(np.transpose(img_torch.numpy(), (1,2,0)).shape)\n",
    "    # feat = extract_deep_feature(np.transpose(img_torch.numpy(), (1,2,0)), vgg16_weights)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\n",
    "        vgg_feat_feat = vgg16.features(img_torch[None, ])\n",
    "        vgg_feat_feat = vgg_classifier(vgg_feat_feat.flatten())\n",
    "    \n",
    "    # return np.sum(np.abs(vgg_feat_feat.numpy() - feat))\n",
    "    # raise NotImplementedError()\n",
    "    N = trained_features.shape[0]\n",
    "    feat = vgg_feat_feat.numpy() #(K,)\n",
    "    feat_repeated = np.stack(([feat]*N)) # (N, K)\n",
    "    distances = np.linalg.norm((feat_repeated-trained_features), axis=1)\n",
    "    idx_min = np.argmin(distances)\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    pred_label = train_labels[idx_min]\n",
    "    return [i, pred_label]\n",
    "\n",
    "def evaluate_recognition_system_vit(vit_feat_extractor):\n",
    "    '''\n",
    "    Evaluates the recognition system for all test images and returns the confusion matrix.\n",
    "\n",
    "    [input]\n",
    "    * vit_feat_extractor: feature extractor for ViT\n",
    "\n",
    "    [output]\n",
    "    * conf: numpy.ndarray of shape (8,8)\n",
    "    * accuracy: accuracy of the evaluated system\n",
    "    '''\n",
    "\n",
    "    test_data = np.load(\"./data/test_data.npz\", allow_pickle=True)\n",
    "\n",
    "    # ----- TODO -----\n",
    "    trained_system = np.load(\"trained_system_vit.npz\", allow_pickle=True)\n",
    "    image_names = test_data['files']\n",
    "    test_labels = test_data['labels']\n",
    "\n",
    "    trained_features = trained_system['features']\n",
    "    train_labels = trained_system['labels']\n",
    "\n",
    "    print(\"Trained features shape: \", trained_features.shape)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Similar approach as Q4.2.1\n",
    "    2.> Do a for loop here instead of multiprocessing\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    num_images = image_names.shape[0]\n",
    "    features_list = []\n",
    "    list_of_args = []\n",
    "    labels_list = []\n",
    "    for i in range(num_images):\n",
    "        full_image_name = './data/' + image_names[i]\n",
    "        image = io.imread(full_image_name) / 255\n",
    "        features_list.append(vit_feat_extractor.extract_feature(image))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        full_image_name = './data/' + image_names[i]\n",
    "        labels_list.append(evaluate_test_image(arg))\n",
    "        \n",
    "    print(len(labels_list))\n",
    "    ordered_labels = labels_list\n",
    "    \n",
    "    conf_matrix = np.zeros((8,8))\n",
    "    for (actual_label, pred_label) in zip(test_labels, ordered_labels):\n",
    "        conf_matrix[actual_label][pred_label]+=1\n",
    "        \n",
    "    accuracy = np.trace(conf_matrix)/np.sum(conf_matrix)\n",
    "    \n",
    "    np.save(\"./trained_conf_matrix.npy\",conf_matrix)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Same code as Q4.2.1, just copy it over\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    np.save(\"./trained_conf_matrix_vit.npy\",conf_matrix)\n",
    "    return conf_matrix, accuracy\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd8e77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "feature_extract_hook() missing 2 required positional arguments: 'input' and 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-de189a465622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mViT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'B_16_imagenet1k'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvit_feat_extractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mbuild_recognition_system_vit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvit_feat_extractor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# conf_matrix, accuracy = evaluate_recognition_system_vit(vit_feat_extractor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-adc307440842>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, layername)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'transformer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extract_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: feature_extract_hook() missing 2 required positional arguments: 'input' and 'output'"
     ]
    }
   ],
   "source": [
    "# NOTE: comment out the lines below before submitting to gradescope\n",
    "### Run the code\n",
    "vit = ViT('B_16_imagenet1k', pretrained=True)\n",
    "vit.eval()\n",
    "vit_feat_extractor = FeatureExtractor(vit)\n",
    "build_recognition_system_vit(vit_feat_extractor)\n",
    "# conf_matrix, accuracy = evaluate_recognition_system_vit(vit_feat_extractor)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c067d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "139b99f8d6775c060c31a5fd340b9143",
     "grade": true,
     "grade_id": "q_4_1_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = np.load('trained_conf_matrix.npy')\n",
    "# print(conf_matrix)\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df_cm = pd.DataFrame(conf_matrix, index = [i for i in range(8)],\n",
    "                  columns = [i for i in range(8)])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da44bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c7cf16f5994d47f23c4c127f7a318a5",
     "grade": true,
     "grade_id": "q_4_1_2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cd3b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce812af4b046483ab1e43c6792b4448d",
     "grade": true,
     "grade_id": "q_4_1_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232d06c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d4948aa8f9c3ce2b4181d9919d68b1",
     "grade": true,
     "grade_id": "q_4_1_4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4d510",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a0e96d948d22209c3b3a5626ae40b6",
     "grade": true,
     "grade_id": "q_4_1_5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfc3c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7555970fe39f565b777d4c8f37478396",
     "grade": true,
     "grade_id": "q_4_1_6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecd654",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40e815d75f7923b2bc81403f5cf53bc8",
     "grade": true,
     "grade_id": "q_4_2_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51bfcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae0796b60f35e32d4449672587cdc6ea",
     "grade": true,
     "grade_id": "q_4_3_1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e257837",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8772a12ca5fc97a77ef86939fc574174",
     "grade": true,
     "grade_id": "q_4_3_2",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6ad70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "877ef725bb5eae4c6cf7678e093d7ffd",
     "grade": true,
     "grade_id": "q_4_3_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19103772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('cvb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c21fd19af84997c7324eb40f3f35b9c516eb0e316e912022307cacada437db6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
